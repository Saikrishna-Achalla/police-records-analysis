{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "0b235fe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages (3.141.0)\r\n",
      "Requirement already satisfied: urllib3 in /Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.26.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "# Options for the driver\n",
    "options = Options()\n",
    "options.headless = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339360a",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "- Figure out if [Selenium Grid](https://www.selenium.dev/documentation/grid/) can potentially improve the performance of the scraper\n",
    "- Run on Google Colab/Microsoft Azure/local desktop/some other place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16ffa5b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-b2026be2226a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-b2026be2226a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    . # Keep this cell to prevent the rest of the notebook from automatically running\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ". # Keep this cell to prevent the rest of the notebook from automatically running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025bda3",
   "metadata": {},
   "source": [
    "The following script uses the `selenium` library to, in theory, scrape every request from [San Diego's NextRequest database](https://sandiego.nextrequest.com/requests). It does so by using the fact that every request has its own unique URL i.e. the request with ID 'yy-xxxx' will be found at '.../requests/yy-xxxx'. From each request webpage, the following information is extracted:\n",
    "\n",
    "- `id` (str): ID of the request, yy-xxxx\n",
    "- `status` (str): Whether the request is opened or closed. Always takes on a value of either 'closed' or 'open'\n",
    "- `desc` (str): Description of the request provided by the requester\n",
    "- `date` (str): Initial request date\n",
    "- `depts` (str): Current departments assigned to the request (may not be the ones the requester had initially)\n",
    "- `docs` (DataFrame in CSV format): All documents attached to the request, if there are any, otherwise None. The columns are:\n",
    "    - `title` (str): Title given to each document\n",
    "    - `link` (str): Link to each document\n",
    "- `poc` (str): Point of contact\n",
    "- `msgs` (DataFrame in CSV format): All messages attached to the request. The columns are:\n",
    "    - `title` (str): Title of each message\n",
    "    - `item` (str): Message body\n",
    "    - `time` (str): Date of each message\n",
    "\n",
    "After a request is scraped, the next request can be navigated to by clicking on an arrow, and the scraper continues to run until the arrow cannot be found, either due to a timeout or because the scraper has reached the last request in the database. To address these potential timeouts, we stop the driver every time it cannot access a request, then restart it after a short delay, starting from the request that it timed out on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "b63e297a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "-----------\n",
      "Starting request: 19-2477\n",
      "\n",
      "Requests scraped: 100 \tAvg runtime: 2.17s/request \tTotal runtime: 216.9s\n",
      "Requests scraped: 200 \tAvg runtime: 2.18s/request \tTotal runtime: 435.6s\n",
      "Requests scraped: 300 \tAvg runtime: 2.17s/request \tTotal runtime: 651.6s\n",
      "Requests scraped: 400 \tAvg runtime: 2.16s/request \tTotal runtime: 864.2s\n",
      "Requests scraped: 500 \tAvg runtime: 2.19s/request \tTotal runtime: 1093.0s\n",
      "Requests scraped: 600 \tAvg runtime: 2.17s/request \tTotal runtime: 1299.9s\n",
      "Requests scraped: 700 \tAvg runtime: 2.17s/request \tTotal runtime: 1518.9s\n",
      "Requests scraped: 800 \tAvg runtime: 2.18s/request \tTotal runtime: 1743.0s\n",
      "Total requests scraped: 816 \tAvg runtime: 2.22s/request \tTotal runtime: 1814.7s\n",
      "\n",
      "Last request scraped: 19-3292\n",
      "\n",
      "Iteration 2\n",
      "-----------\n",
      "Starting request: 19-3292\n",
      "\n",
      "Total requests scraped: 19 \tAvg runtime: 2.91s/request \tTotal runtime: 55.3s\n",
      "\n",
      "Last request scraped: 19-3310\n",
      "\n",
      "Iteration 3\n",
      "-----------\n",
      "Starting request: 19-3310\n",
      "\n",
      "Requests scraped: 100 \tAvg runtime: 2.15s/request \tTotal runtime: 214.8s\n",
      "Requests scraped: 200 \tAvg runtime: 2.13s/request \tTotal runtime: 425.6s\n",
      "Requests scraped: 300 \tAvg runtime: 2.14s/request \tTotal runtime: 643.2s\n",
      "Requests scraped: 400 \tAvg runtime: 2.16s/request \tTotal runtime: 864.6s\n",
      "Requests scraped: 500 \tAvg runtime: 2.18s/request \tTotal runtime: 1087.9s\n",
      "Requests scraped: 600 \tAvg runtime: 2.18s/request \tTotal runtime: 1307.3s\n",
      "Requests scraped: 700 \tAvg runtime: 2.18s/request \tTotal runtime: 1525.1s\n",
      "Requests scraped: 800 \tAvg runtime: 2.18s/request \tTotal runtime: 1742.5s\n",
      "Requests scraped: 900 \tAvg runtime: 2.18s/request \tTotal runtime: 1965.5s\n",
      "Requests scraped: 1000 \tAvg runtime: 2.19s/request \tTotal runtime: 2193.8s\n",
      "Requests scraped: 1100 \tAvg runtime: 2.19s/request \tTotal runtime: 2411.4s\n",
      "Requests scraped: 1200 \tAvg runtime: 2.2s/request \tTotal runtime: 2641.0s\n",
      "Requests scraped: 1300 \tAvg runtime: 2.21s/request \tTotal runtime: 2868.1s\n",
      "Requests scraped: 1400 \tAvg runtime: 2.21s/request \tTotal runtime: 3089.1s\n",
      "Requests scraped: 1500 \tAvg runtime: 2.2s/request \tTotal runtime: 3304.5s\n",
      "Requests scraped: 1600 \tAvg runtime: 2.2s/request \tTotal runtime: 3523.1s\n",
      "Requests scraped: 1700 \tAvg runtime: 2.2s/request \tTotal runtime: 3748.0s\n",
      "Requests scraped: 1800 \tAvg runtime: 2.2s/request \tTotal runtime: 3968.5s\n",
      "Requests scraped: 1900 \tAvg runtime: 2.21s/request \tTotal runtime: 4196.0s\n",
      "Requests scraped: 2000 \tAvg runtime: 2.21s/request \tTotal runtime: 4423.7s\n",
      "Requests scraped: 2100 \tAvg runtime: 2.21s/request \tTotal runtime: 4645.9s\n",
      "Requests scraped: 2200 \tAvg runtime: 2.21s/request \tTotal runtime: 4867.3s\n",
      "Requests scraped: 2300 \tAvg runtime: 2.21s/request \tTotal runtime: 5086.6s\n",
      "Requests scraped: 2400 \tAvg runtime: 2.21s/request \tTotal runtime: 5314.9s\n",
      "Requests scraped: 2500 \tAvg runtime: 2.21s/request \tTotal runtime: 5534.2s\n",
      "Requests scraped: 2600 \tAvg runtime: 2.22s/request \tTotal runtime: 5762.7s\n",
      "Requests scraped: 2700 \tAvg runtime: 2.22s/request \tTotal runtime: 5990.8s\n",
      "Requests scraped: 2800 \tAvg runtime: 2.22s/request \tTotal runtime: 6213.2s\n",
      "Requests scraped: 2900 \tAvg runtime: 2.22s/request \tTotal runtime: 6435.8s\n",
      "Requests scraped: 3000 \tAvg runtime: 2.22s/request \tTotal runtime: 6662.2s\n",
      "Requests scraped: 3100 \tAvg runtime: 2.22s/request \tTotal runtime: 6888.5s\n",
      "Requests scraped: 3200 \tAvg runtime: 2.23s/request \tTotal runtime: 7127.1s\n",
      "Requests scraped: 3300 \tAvg runtime: 2.23s/request \tTotal runtime: 7358.1s\n",
      "Requests scraped: 3400 \tAvg runtime: 2.23s/request \tTotal runtime: 7586.3s\n",
      "Requests scraped: 3500 \tAvg runtime: 2.23s/request \tTotal runtime: 7804.8s\n",
      "Requests scraped: 3600 \tAvg runtime: 2.23s/request \tTotal runtime: 8037.6s\n",
      "Requests scraped: 3700 \tAvg runtime: 2.23s/request \tTotal runtime: 8252.4s\n",
      "Requests scraped: 3800 \tAvg runtime: 2.23s/request \tTotal runtime: 8481.9s\n",
      "Requests scraped: 3900 \tAvg runtime: 2.23s/request \tTotal runtime: 8701.6s\n",
      "Requests scraped: 4000 \tAvg runtime: 2.24s/request \tTotal runtime: 8967.6s\n",
      "Requests scraped: 4100 \tAvg runtime: 2.24s/request \tTotal runtime: 9188.9s\n",
      "Requests scraped: 4200 \tAvg runtime: 2.24s/request \tTotal runtime: 9416.4s\n",
      "Requests scraped: 4300 \tAvg runtime: 2.24s/request \tTotal runtime: 9642.3s\n",
      "Requests scraped: 4400 \tAvg runtime: 2.24s/request \tTotal runtime: 9870.1s\n",
      "Requests scraped: 4500 \tAvg runtime: 2.24s/request \tTotal runtime: 10094.9s\n",
      "Requests scraped: 4600 \tAvg runtime: 2.24s/request \tTotal runtime: 10316.8s\n",
      "Requests scraped: 4700 \tAvg runtime: 2.24s/request \tTotal runtime: 10541.4s\n",
      "Requests scraped: 4800 \tAvg runtime: 2.24s/request \tTotal runtime: 10757.8s\n",
      "Requests scraped: 4900 \tAvg runtime: 2.24s/request \tTotal runtime: 10977.9s\n",
      "Requests scraped: 5000 \tAvg runtime: 2.24s/request \tTotal runtime: 11200.8s\n",
      "Requests scraped: 5100 \tAvg runtime: 2.24s/request \tTotal runtime: 11427.5s\n",
      "Requests scraped: 5200 \tAvg runtime: 2.24s/request \tTotal runtime: 11646.9s\n",
      "Requests scraped: 5300 \tAvg runtime: 2.24s/request \tTotal runtime: 11875.7s\n",
      "Requests scraped: 5400 \tAvg runtime: 2.25s/request \tTotal runtime: 12132.1s\n",
      "Requests scraped: 5500 \tAvg runtime: 2.25s/request \tTotal runtime: 12353.3s\n",
      "Requests scraped: 5600 \tAvg runtime: 2.24s/request \tTotal runtime: 12569.9s\n",
      "Requests scraped: 5700 \tAvg runtime: 2.24s/request \tTotal runtime: 12790.1s\n",
      "Requests scraped: 5800 \tAvg runtime: 2.24s/request \tTotal runtime: 13005.4s\n",
      "Requests scraped: 5900 \tAvg runtime: 2.24s/request \tTotal runtime: 13227.2s\n",
      "Total requests scraped: 5935 \tAvg runtime: 2.24s/request \tTotal runtime: 13310.4s\n",
      "\n",
      "Last request scraped: 20-3126\n",
      "\n",
      "Iteration 4\n",
      "-----------\n",
      "Starting request: 20-3126\n",
      "\n",
      "Requests scraped: 100 \tAvg runtime: 2.38s/request \tTotal runtime: 237.8s\n",
      "Requests scraped: 200 \tAvg runtime: 2.27s/request \tTotal runtime: 453.9s\n",
      "Requests scraped: 300 \tAvg runtime: 2.28s/request \tTotal runtime: 683.1s\n",
      "Requests scraped: 400 \tAvg runtime: 2.26s/request \tTotal runtime: 905.2s\n",
      "Requests scraped: 500 \tAvg runtime: 2.26s/request \tTotal runtime: 1131.4s\n",
      "Requests scraped: 600 \tAvg runtime: 2.25s/request \tTotal runtime: 1349.6s\n",
      "Requests scraped: 700 \tAvg runtime: 2.25s/request \tTotal runtime: 1576.1s\n",
      "Requests scraped: 800 \tAvg runtime: 2.24s/request \tTotal runtime: 1792.9s\n",
      "Requests scraped: 900 \tAvg runtime: 2.24s/request \tTotal runtime: 2020.0s\n",
      "Requests scraped: 1000 \tAvg runtime: 2.24s/request \tTotal runtime: 2243.3s\n",
      "Requests scraped: 1100 \tAvg runtime: 2.25s/request \tTotal runtime: 2469.8s\n",
      "Requests scraped: 1200 \tAvg runtime: 2.25s/request \tTotal runtime: 2695.8s\n",
      "Requests scraped: 1300 \tAvg runtime: 2.25s/request \tTotal runtime: 2919.9s\n",
      "Requests scraped: 1400 \tAvg runtime: 2.25s/request \tTotal runtime: 3148.4s\n",
      "Requests scraped: 1500 \tAvg runtime: 2.25s/request \tTotal runtime: 3368.1s\n",
      "Requests scraped: 1600 \tAvg runtime: 2.24s/request \tTotal runtime: 3588.3s\n",
      "Requests scraped: 1700 \tAvg runtime: 2.24s/request \tTotal runtime: 3813.5s\n",
      "Requests scraped: 1800 \tAvg runtime: 2.24s/request \tTotal runtime: 4036.8s\n",
      "Requests scraped: 1900 \tAvg runtime: 2.24s/request \tTotal runtime: 4254.2s\n",
      "Requests scraped: 2000 \tAvg runtime: 2.24s/request \tTotal runtime: 4472.0s\n",
      "Requests scraped: 2100 \tAvg runtime: 2.24s/request \tTotal runtime: 4694.0s\n",
      "Requests scraped: 2200 \tAvg runtime: 2.23s/request \tTotal runtime: 4913.0s\n",
      "Requests scraped: 2300 \tAvg runtime: 2.23s/request \tTotal runtime: 5133.7s\n",
      "Requests scraped: 2400 \tAvg runtime: 2.23s/request \tTotal runtime: 5355.4s\n",
      "Requests scraped: 2500 \tAvg runtime: 2.23s/request \tTotal runtime: 5577.8s\n",
      "Requests scraped: 2600 \tAvg runtime: 2.23s/request \tTotal runtime: 5808.6s\n",
      "Total requests scraped: 2646 \tAvg runtime: 2.24s/request \tTotal runtime: 5928.5s\n",
      "\n",
      "Last request scraped: 20-5808\n",
      "\n",
      "Iteration 5\n",
      "-----------\n",
      "Starting request: 20-5808\n",
      "\n",
      "Requests scraped: 100 \tAvg runtime: 2.26s/request \tTotal runtime: 226.1s\n",
      "Requests scraped: 200 \tAvg runtime: 2.29s/request \tTotal runtime: 458.1s\n",
      "Requests scraped: 300 \tAvg runtime: 2.28s/request \tTotal runtime: 682.6s\n",
      "Requests scraped: 400 \tAvg runtime: 2.26s/request \tTotal runtime: 905.7s\n",
      "Requests scraped: 500 \tAvg runtime: 2.27s/request \tTotal runtime: 1133.2s\n",
      "Requests scraped: 600 \tAvg runtime: 2.27s/request \tTotal runtime: 1359.2s\n",
      "Requests scraped: 700 \tAvg runtime: 2.26s/request \tTotal runtime: 1583.4s\n",
      "Requests scraped: 800 \tAvg runtime: 2.26s/request \tTotal runtime: 1805.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests scraped: 900 \tAvg runtime: 2.26s/request \tTotal runtime: 2029.8s\n",
      "Requests scraped: 1000 \tAvg runtime: 2.25s/request \tTotal runtime: 2253.9s\n",
      "Requests scraped: 1100 \tAvg runtime: 2.25s/request \tTotal runtime: 2476.0s\n",
      "Requests scraped: 1200 \tAvg runtime: 2.26s/request \tTotal runtime: 2710.8s\n",
      "Requests scraped: 1300 \tAvg runtime: 2.27s/request \tTotal runtime: 2947.1s\n",
      "Total requests scraped: 1367 \tAvg runtime: 2.27s/request \tTotal runtime: 3108.4s\n",
      "\n",
      "Last request scraped: 21-1378\n",
      "\n",
      "Iteration 6\n",
      "-----------\n",
      "Starting request: 21-1378\n",
      "\n",
      "Requests scraped: 100 \tAvg runtime: 2.3s/request \tTotal runtime: 230.3s\n",
      "Requests scraped: 200 \tAvg runtime: 2.29s/request \tTotal runtime: 457.8s\n",
      "Requests scraped: 300 \tAvg runtime: 2.27s/request \tTotal runtime: 681.0s\n",
      "Total requests scraped: 335 \tAvg runtime: 2.29s/request \tTotal runtime: 768.1s\n",
      "\n",
      "Last request scraped: 21-1730\n",
      "\n",
      "Iteration 7\n",
      "-----------\n",
      "Starting request: 21-1730\n",
      "\n",
      "Requests scraped: 100 \tAvg runtime: 2.21s/request \tTotal runtime: 221.0s\n",
      "Requests scraped: 200 \tAvg runtime: 2.25s/request \tTotal runtime: 449.6s\n",
      "Requests scraped: 300 \tAvg runtime: 2.26s/request \tTotal runtime: 678.7s\n",
      "Requests scraped: 400 \tAvg runtime: 2.26s/request \tTotal runtime: 903.0s\n",
      "Requests scraped: 500 \tAvg runtime: 2.26s/request \tTotal runtime: 1131.7s\n",
      "Requests scraped: 600 \tAvg runtime: 2.34s/request \tTotal runtime: 1404.4s\n",
      "Requests scraped: 700 \tAvg runtime: 2.32s/request \tTotal runtime: 1625.5s\n",
      "Requests scraped: 800 \tAvg runtime: 2.31s/request \tTotal runtime: 1849.8s\n",
      "Requests scraped: 900 \tAvg runtime: 2.31s/request \tTotal runtime: 2082.7s\n",
      "Requests scraped: 1000 \tAvg runtime: 2.31s/request \tTotal runtime: 2307.0s\n",
      "Requests scraped: 1100 \tAvg runtime: 2.31s/request \tTotal runtime: 2535.8s\n",
      "Requests scraped: 1200 \tAvg runtime: 2.32s/request \tTotal runtime: 2786.9s\n",
      "Requests scraped: 1300 \tAvg runtime: 2.33s/request \tTotal runtime: 3026.7s\n",
      "Requests scraped: 1400 \tAvg runtime: 2.32s/request \tTotal runtime: 3254.2s\n",
      "Requests scraped: 1500 \tAvg runtime: 2.32s/request \tTotal runtime: 3474.5s\n",
      "Requests scraped: 1600 \tAvg runtime: 2.31s/request \tTotal runtime: 3694.6s\n",
      "Requests scraped: 1700 \tAvg runtime: 2.31s/request \tTotal runtime: 3923.6s\n",
      "Requests scraped: 1800 \tAvg runtime: 2.3s/request \tTotal runtime: 4145.1s\n",
      "Requests scraped: 1900 \tAvg runtime: 2.3s/request \tTotal runtime: 4371.9s\n",
      "Requests scraped: 2000 \tAvg runtime: 2.31s/request \tTotal runtime: 4620.8s\n",
      "Requests scraped: 2100 \tAvg runtime: 2.31s/request \tTotal runtime: 4849.3s\n",
      "Requests scraped: 2200 \tAvg runtime: 2.31s/request \tTotal runtime: 5086.1s\n",
      "Requests scraped: 2300 \tAvg runtime: 2.32s/request \tTotal runtime: 5327.6s\n",
      "Requests scraped: 2400 \tAvg runtime: 2.32s/request \tTotal runtime: 5561.6s\n",
      "Requests scraped: 2500 \tAvg runtime: 2.32s/request \tTotal runtime: 5791.8s\n",
      "Requests scraped: 2600 \tAvg runtime: 2.32s/request \tTotal runtime: 6024.9s\n",
      "Requests scraped: 2700 \tAvg runtime: 2.32s/request \tTotal runtime: 6251.9s\n",
      "Requests scraped: 2800 \tAvg runtime: 2.32s/request \tTotal runtime: 6483.0s\n",
      "Requests scraped: 2900 \tAvg runtime: 2.31s/request \tTotal runtime: 6708.4s\n",
      "Requests scraped: 3000 \tAvg runtime: 2.31s/request \tTotal runtime: 6937.0s\n",
      "Requests scraped: 3100 \tAvg runtime: 2.31s/request \tTotal runtime: 7164.6s\n",
      "Exception encountered:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-622-6acee892921f>\", line 22, in scrape_record_append\n",
      "    folder.click()\n",
      "  File \"/Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webelement.py\", line 80, in click\n",
      "    self._execute(Command.CLICK_ELEMENT)\n",
      "  File \"/Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webelement.py\", line 633, in _execute\n",
      "    return self._parent.execute(command, params)\n",
      "  File \"/Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 321, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"/Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\", line 242, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.StaleElementReferenceException: Message: The element reference of <a class=\"icon-edit icon-edit-inline folder-toggle\" href=\"#\"> is stale; either the element is no longer attached to the DOM, it is not in the current frame context, or the document has been refreshed\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests scraped: 3200 \tAvg runtime: 2.31s/request \tTotal runtime: 7403.2s\n",
      "Requests scraped: 3300 \tAvg runtime: 2.31s/request \tTotal runtime: 7624.6s\n",
      "Requests scraped: 3400 \tAvg runtime: 2.31s/request \tTotal runtime: 7849.0s\n",
      "Requests scraped: 3500 \tAvg runtime: 2.31s/request \tTotal runtime: 8072.9s\n",
      "Total requests scraped: 3572 \tAvg runtime: 2.3s/request \tTotal runtime: 8227.5s\n",
      "\n",
      "Last request scraped: 21-5614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the scraper\n",
    "urls = ['https://sandiego.nextrequest.com/requests/', 'https://oaklandca.nextrequest.com/requests/'] # URLs\n",
    "i = 0 # Index of URL to scrape from\n",
    "current_id = sd_requests[-1]['id'] # The current ID, initialized to the ID to start scraping from\n",
    "num_requests = -1 # Number of requests to scrape\n",
    "cooldown = 1 # Cooldown between request accesses\n",
    "timeout = 10 # Timeout wait time between scraper runs\n",
    "progress = 100 # Number of requests to show progress for\n",
    "num_runs = 1 # Keeps track of how many times the scraper has been run\n",
    "\n",
    "driver = webdriver.Firefox(options=options) # Instantiate headless (non-visible) Firefox driver\n",
    "\n",
    "# Run an intial iteration of the scraper\n",
    "driver.get(urls[i] + current_id)\n",
    "\n",
    "# Print iteration number\n",
    "it_num_title = 'Iteration ' + str(num_runs)\n",
    "print(it_num_title)\n",
    "print('-' * len(it_num_title))\n",
    "\n",
    "# Re-scrape the current request\n",
    "print('Starting request:', sd_requests.pop()['id']) \n",
    "print()\n",
    "\n",
    "# Scrape requests until it either reaches the end or times out\n",
    "scrape_requests_sequential(sd_requests, driver, \n",
    "                           num_requests=num_requests, \n",
    "                           cooldown=cooldown, \n",
    "                           progress=progress)\n",
    "\n",
    "num_runs += 1\n",
    "sleep(timeout) # Wait after a timeout\n",
    "\n",
    "# Restart the driver at the last request scraped \n",
    "current_id = sd_requests[-1]['id']\n",
    "driver.get(urls[i] + current_id)\n",
    "\n",
    "# Continue to scrape until the arrow to go to the next request is no longer present\n",
    "while driver.find_elements_by_class_name('js-next-request'):\n",
    "    # Scrape requests until the next timeout or the arrow is no longer present\n",
    "    it_num_title = 'Iteration ' + str(num_runs)\n",
    "    print(it_num_title)\n",
    "    print('-' * len(it_num_title))\n",
    "    \n",
    "    print('Starting request:', sd_requests.pop()['id'])\n",
    "    print()\n",
    "    \n",
    "    scrape_requests_sequential(sd_requests, driver, \n",
    "                               num_requests=num_requests, \n",
    "                               cooldown=cooldown, \n",
    "                               progress=progress)\n",
    "    \n",
    "    num_runs += 1\n",
    "    sleep(timeout)\n",
    "\n",
    "    current_id = sd_requests[-1]['id']\n",
    "    driver.get(urls[i] + current_id)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Convert to DataFrame\n",
    "sd_requests = [request for request in sd_requests if (request and request['status'])]\n",
    "sd_requests_df = pd.DataFrame(sd_requests).drop_duplicates()\n",
    "\n",
    "# Create a zipped CSV file of the DataFrame\n",
    "compression_opts = dict(method='zip', archive_name='sd_requests.csv')\n",
    "sd_requests_df.to_csv('data/sd_requests.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "74707787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "sd_requests = [request for request in sd_requests if (request and request['status'])]\n",
    "sd_requests_df = pd.DataFrame(sd_requests).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "0b18c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zipped CSV file of the DataFrame\n",
    "compression_opts = dict(method='zip', archive_name='sd_requests.csv')\n",
    "sd_requests_df.to_csv('data/sd_requests.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "237a8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the CSV file was properly created\n",
    "test_df = pd.read_csv(zipfile.ZipFile('data/sd_requests.zip', 'r').open('sd_requests.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "05b9d8c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status</th>\n",
       "      <th>desc</th>\n",
       "      <th>date</th>\n",
       "      <th>depts</th>\n",
       "      <th>docs</th>\n",
       "      <th>poc</th>\n",
       "      <th>msgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15-1810</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Notices of Violation/Notice to Comply, fire in...</td>\n",
       "      <td>December 7, 2015 via web</td>\n",
       "      <td>Code Enforcement</td>\n",
       "      <td>title,link\\n5040 ShorehamPlace building permit...</td>\n",
       "      <td>Ginger Rodriguez</td>\n",
       "      <td>title,item,time\\n\"Request Closed\\nPublic\",02. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-1811</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>The October 2015 monthly report for SeaWorld</td>\n",
       "      <td>December 7, 2015 via web</td>\n",
       "      <td>Department of Real Estate and Airport Management</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeffrey Wallace</td>\n",
       "      <td>title,item,time\\n\"Request Published\\nPublic\",,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15-1812</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Records related to the following BIDS:  Adams ...</td>\n",
       "      <td>December 7, 2015 via web</td>\n",
       "      <td>City Clerk</td>\n",
       "      <td>title,link\\nhttp://www.sandiego.gov/park-and-r...</td>\n",
       "      <td>Mailei Ross-Cerezo</td>\n",
       "      <td>title,item,time\\n\"Request Closed\\nPublic\",Stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15-1813</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Historical lease payments made by SeaWorld to ...</td>\n",
       "      <td>December 7, 2015 via web</td>\n",
       "      <td>Department of Real Estate and Airport Management</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeffrey Wallace</td>\n",
       "      <td>title,item,time\\n\"Request Closed\\nPublic\",02. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15-1814</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Open violations, variances, ordinances, approv...</td>\n",
       "      <td>December 7, 2015 via web</td>\n",
       "      <td>Code Enforcement</td>\n",
       "      <td>title,link\\n15-1814 Fire Responsive.pdf,https:...</td>\n",
       "      <td>Ginger Rodriguez</td>\n",
       "      <td>title,item,time\\n\"Request Closed\\nPublic\",02. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28842</th>\n",
       "      <td>21-5579</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Hello, I would like a copy of the report from ...</td>\n",
       "      <td>October 28, 2021 via web</td>\n",
       "      <td>Animal Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lori Hernandez</td>\n",
       "      <td>title,item,time\\n\"Request Published\\nPublic\",,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28843</th>\n",
       "      <td>21-5581</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>Requesting incident report and photos for San ...</td>\n",
       "      <td>October 28, 2021 via web</td>\n",
       "      <td>Animal Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lori Hernandez</td>\n",
       "      <td>title,item,time\\n\"Request Published\\nPublic\",,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28844</th>\n",
       "      <td>21-5584</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>request for call for service\\n\\n2110020816</td>\n",
       "      <td>October 28, 2021 via web</td>\n",
       "      <td>Police</td>\n",
       "      <td>title,link\\n2110020816_Redacted.pdf,https://sa...</td>\n",
       "      <td>Lori Hernandez</td>\n",
       "      <td>title,item,time\\n\"Request Published\\nPublic\",,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28845</th>\n",
       "      <td>21-5588</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>request for call for service\\n\\nE20050048015</td>\n",
       "      <td>October 28, 2021 via web</td>\n",
       "      <td>Police</td>\n",
       "      <td>title,link\\nE20050048015_Redacted.pdf,https://...</td>\n",
       "      <td>Lori Hernandez</td>\n",
       "      <td>title,item,time\\n\"Request Published\\nPublic\",,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28846</th>\n",
       "      <td>21-5614</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>To Whom It May Concern,\\n\\nWe are the property...</td>\n",
       "      <td>October 29, 2021 via web</td>\n",
       "      <td>Public Records Administration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Angela Laurita</td>\n",
       "      <td>title,item,time\\n\"Request Published\\nPublic\",,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28847 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  status                                               desc  \\\n",
       "0      15-1810  CLOSED  Notices of Violation/Notice to Comply, fire in...   \n",
       "1      15-1811  CLOSED       The October 2015 monthly report for SeaWorld   \n",
       "2      15-1812  CLOSED  Records related to the following BIDS:  Adams ...   \n",
       "3      15-1813  CLOSED  Historical lease payments made by SeaWorld to ...   \n",
       "4      15-1814  CLOSED  Open violations, variances, ordinances, approv...   \n",
       "...        ...     ...                                                ...   \n",
       "28842  21-5579  CLOSED  Hello, I would like a copy of the report from ...   \n",
       "28843  21-5581  CLOSED  Requesting incident report and photos for San ...   \n",
       "28844  21-5584  CLOSED         request for call for service\\n\\n2110020816   \n",
       "28845  21-5588  CLOSED       request for call for service\\n\\nE20050048015   \n",
       "28846  21-5614  CLOSED  To Whom It May Concern,\\n\\nWe are the property...   \n",
       "\n",
       "                           date  \\\n",
       "0      December 7, 2015 via web   \n",
       "1      December 7, 2015 via web   \n",
       "2      December 7, 2015 via web   \n",
       "3      December 7, 2015 via web   \n",
       "4      December 7, 2015 via web   \n",
       "...                         ...   \n",
       "28842  October 28, 2021 via web   \n",
       "28843  October 28, 2021 via web   \n",
       "28844  October 28, 2021 via web   \n",
       "28845  October 28, 2021 via web   \n",
       "28846  October 29, 2021 via web   \n",
       "\n",
       "                                                  depts  \\\n",
       "0                                      Code Enforcement   \n",
       "1      Department of Real Estate and Airport Management   \n",
       "2                                            City Clerk   \n",
       "3      Department of Real Estate and Airport Management   \n",
       "4                                      Code Enforcement   \n",
       "...                                                 ...   \n",
       "28842                                   Animal Services   \n",
       "28843                                   Animal Services   \n",
       "28844                                            Police   \n",
       "28845                                            Police   \n",
       "28846                     Public Records Administration   \n",
       "\n",
       "                                                    docs                 poc  \\\n",
       "0      title,link\\n5040 ShorehamPlace building permit...    Ginger Rodriguez   \n",
       "1                                                    NaN     Jeffrey Wallace   \n",
       "2      title,link\\nhttp://www.sandiego.gov/park-and-r...  Mailei Ross-Cerezo   \n",
       "3                                                    NaN     Jeffrey Wallace   \n",
       "4      title,link\\n15-1814 Fire Responsive.pdf,https:...    Ginger Rodriguez   \n",
       "...                                                  ...                 ...   \n",
       "28842                                                NaN      Lori Hernandez   \n",
       "28843                                                NaN      Lori Hernandez   \n",
       "28844  title,link\\n2110020816_Redacted.pdf,https://sa...      Lori Hernandez   \n",
       "28845  title,link\\nE20050048015_Redacted.pdf,https://...      Lori Hernandez   \n",
       "28846                                                NaN      Angela Laurita   \n",
       "\n",
       "                                                    msgs  \n",
       "0      title,item,time\\n\"Request Closed\\nPublic\",02. ...  \n",
       "1      title,item,time\\n\"Request Published\\nPublic\",,...  \n",
       "2      title,item,time\\n\"Request Closed\\nPublic\",Stil...  \n",
       "3      title,item,time\\n\"Request Closed\\nPublic\",02. ...  \n",
       "4      title,item,time\\n\"Request Closed\\nPublic\",02. ...  \n",
       "...                                                  ...  \n",
       "28842  title,item,time\\n\"Request Published\\nPublic\",,...  \n",
       "28843  title,item,time\\n\"Request Published\\nPublic\",,...  \n",
       "28844  title,item,time\\n\"Request Published\\nPublic\",,...  \n",
       "28845  title,item,time\\n\"Request Published\\nPublic\",,...  \n",
       "28846  title,item,time\\n\"Request Published\\nPublic\",,...  \n",
       "\n",
       "[28847 rows x 8 columns]"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed037dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "e45a23a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "initial_value must be str or None, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-646-05423ecd8aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcsv_to_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs_df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_to_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msgs_df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msgs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_to_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-646-05423ecd8aaf>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(csv)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsv_to_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs_df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_to_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msgs_df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msgs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_to_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: initial_value must be str or None, not float"
     ]
    }
   ],
   "source": [
    "csv_to_df = lambda csv: pd.read_csv(StringIO(csv)) if csv else None\n",
    "test_df['docs_df'] = test_df['docs'].apply(csv_to_df)\n",
    "test_df['msgs_df'] = test_df['msgs'].apply(csv_to_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ad2ef",
   "metadata": {},
   "source": [
    "Scraper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "bf39626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_requests_sequential(requests, driver, num_requests=-1, cooldown=1, debug=0, progress=0):\n",
    "    '''\n",
    "    Scrapes all records on a NextRequest request database starting from the given ID and\n",
    "    moving forward chronologically until the number of requests scraped reaches a given\n",
    "    number. Each scraped requests is added to a given list. If num_requests is non-positive, \n",
    "    then scrape as many records as possible.\n",
    "    '''\n",
    "    counter = 0 # Keeps track of how many requests have been scraped\n",
    "    start = timer() # Timer for progress checking purposes\n",
    "    \n",
    "    # Start by scraping the initial record\n",
    "    \n",
    "    # Only scrape a request if it was loaded properly; otherwise, stop the scraper\n",
    "    if not driver.find_elements_by_class_name('nextrequest'):\n",
    "        print('No requests scraped')\n",
    "        return counter\n",
    "\n",
    "    scrape_request_append(requests, driver, debug=debug) # Scrape request\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    # For positive num_requests, return the list of requests if the counter reaches the desired number\n",
    "    if ((num_requests > 0) and (counter == num_requests)):\n",
    "        if progress:\n",
    "            end = timer()\n",
    "            print('Total requests scraped:', counter, \n",
    "                  '\\tAvg runtime:', str(round((end - start)/counter, 2)) + 's/request', \n",
    "                  '\\tTotal runtime:', str(round(end - start, 1)) + 's')\n",
    "            print()\n",
    "            print('Last request scraped:', requests[-1]['id'])\n",
    "            print()\n",
    "        \n",
    "        return counter\n",
    "\n",
    "    # Show progress, if desired\n",
    "    if progress and (counter % progress == 0):\n",
    "        end = timer()\n",
    "        print('Requests scraped:', counter, \n",
    "              '\\tAvg runtime:', str(round((end - start)/counter, 2)) + 's/request', \n",
    "              '\\tTotal runtime:', str(round(end - start, 1)) + 's')\n",
    "    \n",
    "    # Continue to scrape until the arrow to go to the next request is no longer present\n",
    "    while driver.find_elements_by_class_name('js-next-request'): \n",
    "        driver.find_element_by_class_name('js-next-request').click() # Click on the arrow to navigate to the next request\n",
    "        sleep(cooldown) # Cooldown between scraping attempts\n",
    "        \n",
    "        # Scrape request\n",
    "        if not driver.find_elements_by_class_name('nextrequest'):\n",
    "            break\n",
    "        \n",
    "        scrape_request_append(requests, driver, debug=debug) \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if ((num_requests > 0) and (counter == num_requests)):\n",
    "            break\n",
    "        \n",
    "        if progress and (counter % progress == 0):\n",
    "            end = timer()\n",
    "            print('Requests scraped:', counter, \n",
    "                  '\\tAvg runtime:', str(round((end - start)/counter, 2)) + 's/request', \n",
    "                  '\\tTotal runtime:', str(round(end - start, 1)) + 's')\n",
    "    \n",
    "    # Final progress check\n",
    "    if progress:\n",
    "        end = timer()\n",
    "        print('Total requests scraped:', counter, \n",
    "              '\\tAvg runtime:', str(round((end - start)/counter, 2)) + 's/request', \n",
    "              '\\tTotal runtime:', str(round(end - start, 1)) + 's')\n",
    "        print()\n",
    "        print('Last request scraped:', requests[-1]['id'])\n",
    "        print()\n",
    "        \n",
    "    return counter\n",
    "\n",
    "def scrape_request_append(requests, driver, debug=0):\n",
    "    '''\n",
    "    Scrapes data about a given request on a NextRequest request database, appending the result\n",
    "    to the given list.\n",
    "    '''\n",
    "    request_id, status, desc, date, depts, docs, poc, events = [None] * 8 # Initialize variables \n",
    "    try: # Attempt to scrape relevant data\n",
    "        request_id = driver.find_element_by_class_name('request-title-text').text.split()[1][1:] # Request ID\n",
    "        status = driver.find_element_by_class_name('request-status-label').text.strip() # Request status\n",
    "        desc = driver.find_element_by_class_name('request-text.row').text # Request description\n",
    "        date = driver.find_element_by_class_name('request_date').text # Request date\n",
    "        depts = driver.find_element_by_class_name('current-department').text # Department(s) assigned to the request\n",
    "        poc = driver.find_element_by_class_name('request-detail').text # Person of contact\n",
    "\n",
    "        # Documents attached to the request, if there are any\n",
    "        public_docs = driver.find_element_by_id('public-docs') # Documents block\n",
    "        if '(none)' not in public_docs.text: # Check for the presence of documents\n",
    "            # Expand folders, if there are any\n",
    "            folders = public_docs.find_elements_by_class_name('folder-toggle') \n",
    "            if folders:\n",
    "                for folder in folders:\n",
    "                    folder.click()\n",
    "\n",
    "            doc_links = public_docs.find_elements_by_class_name('document-link') # Links to documents\n",
    "\n",
    "            # DataFrame-converted-to-CSV consisting of all documents\n",
    "            docs = pd.DataFrame({\n",
    "                'title': get_webelement_text(doc_links),\n",
    "                'link': remove_download_from_urls(get_webelement_link(doc_links))\n",
    "                }).to_csv(index=False)\n",
    "\n",
    "        # Messages recorded on the request page, if there are any\n",
    "        event_history = driver.find_elements_by_class_name('generic-event') # All message blocks\n",
    "        if event_history: # Check for presence of \n",
    "            num_events = len(event_history)\n",
    "\n",
    "            # Titles, descriptions, and time strings for each message\n",
    "            event_titles = [None] * num_events\n",
    "            event_items = [None] * num_events\n",
    "            time_quotes = [None] * num_events\n",
    "\n",
    "            # Scrape information from each individual event\n",
    "            for i in range(len(event_history)):\n",
    "                event = event_history[i]\n",
    "\n",
    "                event_title = event.find_element_by_class_name('event-title').text\n",
    "                event_item = '\\n'.join(get_webelement_text(event.find_elements_by_class_name('event-item'))) # Necessary to address the case where there are multiple event-items\n",
    "                time_quote = event.find_element_by_class_name('time-quotes').text\n",
    "\n",
    "                event_titles[i] = event_title\n",
    "                event_items[i] = event_item\n",
    "                time_quotes[i] = time_quote\n",
    "\n",
    "            # DataFrame-converted-to-CSV consisting of all messages\n",
    "            events = pd.DataFrame({ \n",
    "                'title': event_titles,\n",
    "                'item': event_items,\n",
    "                'time': time_quotes\n",
    "                }).to_csv(index=False)\n",
    "            \n",
    "        # For testing purposes, print a message whenever a request is successfully scraped\n",
    "        if debug:\n",
    "            print(request_id, 'scraped')\n",
    "    except NoSuchElementException: # Catch exception thrown if a specific element cannot be found, and print information about the exception\n",
    "        print('NoSuchElementException encountered:')\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "    except: # If some other exception occurs, do the same\n",
    "        print('Exception encountered:')\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "    finally: # Append the request to the list\n",
    "        requests.append({\n",
    "            'id': request_id,\n",
    "            'status': status,\n",
    "            'desc': desc,\n",
    "            'date': date,\n",
    "            'depts': depts,\n",
    "            'docs': docs,\n",
    "            'poc': poc,\n",
    "            'msgs': events\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16bdbb",
   "metadata": {},
   "source": [
    "Scraper utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "751459bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_from_url(url):\n",
    "    '''\n",
    "    Finds the city name from the NextRequest URL.\n",
    "    '''\n",
    "    return re.match(r'(?<=https://)[a-zA-Z]*', url)[0]\n",
    "\n",
    "def get_webelement_text(webelement):\n",
    "    '''\n",
    "    Gets the text of each web element in a list, if such a list exists.\n",
    "    '''\n",
    "    return list(map(lambda x: x.text, webelement)) if webelement else []\n",
    "\n",
    "def get_webelement_link(webelement):\n",
    "    '''\n",
    "    Gets the link of each web element in a list, if such a list exists.\n",
    "    '''\n",
    "    return list(map(lambda x: x.get_attribute('href'), webelement)) if webelement else []\n",
    "\n",
    "def remove_download_from_urls(urls):\n",
    "    '''\n",
    "    Removes '/download' from the end of a list of URLs, if the list exists.\n",
    "    '''\n",
    "    return list(map(lambda url: re.match(r'.*(?=/download)', url)[0], urls)) if urls else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68316b8f",
   "metadata": {},
   "source": [
    "Previous scraper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "0e483b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_record(url, request_id, driver):\n",
    "    '''\n",
    "    Scrapes data about a given request on a NextRequest request database\n",
    "    '''\n",
    "    driver.get(url + request_id) # Attempt to access the record\n",
    "#     timeout = 2 # Timeout length, in seconds\n",
    "    \n",
    "#     try:\n",
    "#         WebDriverWait(driver, timeout).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, 'nextrequest')))\n",
    "#     except TimeoutException:\n",
    "#         print(request_id, 'timed out')\n",
    "#         pass\n",
    "    \n",
    "    if (request_id not in driver.title):\n",
    "        return\n",
    "    \n",
    "    status, desc, date, depts, docs, poc, events = [None] * 7 # Initialize variables \n",
    "    try: # Attempt to scrape relevant data\n",
    "        status = driver.find_element_by_class_name('request-status-label').text.strip() # Request status\n",
    "        desc = driver.find_element_by_class_name('request-text.row').text # Request description\n",
    "        date = driver.find_element_by_class_name('request_date').text # Request date\n",
    "        depts = driver.find_element_by_class_name('current-department').text # Department(s) assigned to the request\n",
    "        poc = driver.find_element_by_class_name('request-detail').text # Person of contact\n",
    "        \n",
    "        # Documents attached to the request, if there are any\n",
    "        public_docs = driver.find_element_by_id('public-docs') # WebElement containing the documents\n",
    "        if '(none)' not in public_docs.text:\n",
    "            # Expand folders, if there are any\n",
    "            folders = public_docs.find_elements_by_class_name('folder-toggle') \n",
    "            if folders:\n",
    "                for folder in folders:\n",
    "                    folder.click()\n",
    "\n",
    "            doc_links = public_docs.find_elements_by_class_name('document-link') # Links to documents\n",
    "            \n",
    "            # DataFrame consisting of all documents\n",
    "            docs = pd.DataFrame({\n",
    "                'title': get_webelement_text(doc_links),\n",
    "                'link': remove_download_from_urls(get_webelement_link(doc_links))\n",
    "                })\n",
    "            \n",
    "        # Messages recorded on the request page, if there are any\n",
    "        event_history = driver.find_elements_by_class_name('generic-event')\n",
    "        if event_history:\n",
    "            num_events = len(event_history)\n",
    "            \n",
    "            # Titles, descriptions, and time strings for each message\n",
    "            event_titles = [None] * num_events\n",
    "            event_items = [None] * num_events\n",
    "            time_quotes = [None] * num_events\n",
    "            \n",
    "            # Scrape information from each individual event\n",
    "            for i in range(len(event_history)):\n",
    "                event = event_history[i]\n",
    "                \n",
    "                event_title = event.find_element_by_class_name('event-title').text\n",
    "                event_item = '\\n'.join(get_webelement_text(event.find_elements_by_class_name('event-item')))\n",
    "                time_quote = event.find_element_by_class_name('time-quotes').text\n",
    "                \n",
    "                event_titles[i] = event_title\n",
    "                event_items[i] = event_item\n",
    "                time_quotes[i] = time_quote\n",
    "                \n",
    "            # DataFrame consisting of all messages\n",
    "            events = pd.DataFrame({ \n",
    "                'title': event_titles,\n",
    "                'item': event_items,\n",
    "                'time': time_quotes\n",
    "                })\n",
    "    except NoSuchElementException: # Catch exception thrown if a specific element cannot be found, and silently pass\n",
    "        pass\n",
    "    except: # If some other exception occurs, print information about the exception\n",
    "        traceback.print_exc()\n",
    "    finally: # Return the request\n",
    "        return {\n",
    "            'id': request_id,\n",
    "            'status': status,\n",
    "            'desc': desc,\n",
    "            'date': date,\n",
    "            'depts': depts,\n",
    "            'docs': docs,\n",
    "            'poc': poc,\n",
    "            'msgs': events\n",
    "            }\n",
    "    \n",
    "def scrape_record_parallel(url, request_id):\n",
    "    '''\n",
    "    Scraper method used for parallelization\n",
    "    '''\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    request_info = scrape_record(url, request_id, driver)\n",
    "    driver.close()\n",
    "    return request_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "294374c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for scraping\n",
    "earliest_year = 16 # Earliest year to search requests for\n",
    "latest_year = 21 # Latest year to search request for\n",
    "id_start = 1 # ID value to start from\n",
    "id_range = id_max # Number of IDs to try for each year\n",
    "cooldown = 0.9 # Amount of time, in seconds, to wait between website accesses\n",
    "\n",
    "start_id = '15-1810' # The request to start scraping from\n",
    "num_requests = -1 # Number of requests to scrape\n",
    "progress = 100 # Display a message every 100 requests successfully scraped\n",
    "\n",
    "# List of all request IDs\n",
    "request_ids = [str(year) + '-' + str(num) for num in range(id_start, id_range + id_start) \n",
    "                                       for year in range(earliest_year, latest_year + 1)]\n",
    "\n",
    "# URLs to extract data from\n",
    "urls = ['https://sandiego.nextrequest.com/requests/', 'https://oaklandca.nextrequest.com/requests/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "253e2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# Iterative script\n",
    "driver = webdriver.Firefox(options=options\n",
    "                          ) # Headless (non-visible) WebDriver\n",
    "\n",
    "sd_requests = [] # List of dictionaries containing information on each request\n",
    "i = 0 # Index for URLs - may be useful later for scraping multiple sites\n",
    "\n",
    "for year in range(earliest_year, latest_year + 1):\n",
    "    for num in tqdm(range(id_start, id_range + id_start)):\n",
    "        # NextRequest request IDs are a two-digit year and a number, with a dash in between\n",
    "        request_id = str(year) + '-' + str(num)\n",
    "        \n",
    "        # Scrape record\n",
    "        sd_requests.append(scrape_record(urls[i], request_id, driver)) \n",
    "\n",
    "        # Cooldown\n",
    "        sleep(cooldown)\n",
    "    \n",
    "    # sleep(cooldown) # Cooldown\n",
    "        \n",
    "driver.close()\n",
    "\n",
    "sd_requests = [x for x in sd_requests if x['status'] is not None] # Remove entries with incomplete information\n",
    "sd_requests_df = pd.DataFrame(sd_requests) # Convert to DataFrame\n",
    "\n",
    "# Create a zipped CSV file of the data\n",
    "compression_opts = dict(method='zip', archive_name='sd_requests.csv')\n",
    "sd_requests_df.to_csv('data/sd_requests_2.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5fc71",
   "metadata": {},
   "source": [
    "Miscellaneous tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cca844da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test remote WebDriver (run Selenium Grid instance locally before running this cell)\n",
    "# driver = webdriver.Remote(desired_capabilities=DesiredCapabilities.FIREFOX, options=options)\n",
    "# driver.get('http://www.google.com')\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0a275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Test to make sure the driver works\n",
    "# \n",
    "# driver = webdriver.Firefox(options=options) # Instantiate a headless Firefox WebDriver\n",
    "# for url in urls:\n",
    "#     driver.get(url)\n",
    "#     print(driver.title)\n",
    "#     sleep(5)\n",
    "    \n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8220c94",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# # Test for retrieving message info from a specific request\n",
    "# driver = webdriver.Firefox(options=options)\n",
    "# driver.get('https://sandiego.nextrequest.com/requests/21-4915')\n",
    "# print(driver.title)\n",
    "\n",
    "# event_titles = driver.find_elements_by_class_name('event-title')\n",
    "# event_items = driver.find_elements_by_class_name('event-item')\n",
    "# times = driver.find_elements_by_class_name('time-quotes')\n",
    "# for title, item, time in list(zip(event_titles, event_items, times)):\n",
    "#     print(title.text)\n",
    "#     print(item.text)\n",
    "#     print(time.text)\n",
    "#     print()\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96755c02",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Scraping documents test\n",
    "# driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# for request_id in [5369, 5313, 5374]:\n",
    "#     url = 'https://sandiego.nextrequest.com/requests/21-' + str(request_id)\n",
    "#     driver.get(url)\n",
    "\n",
    "#     docs = driver.find_element_by_id('public-docs')\n",
    "#     folders = docs.find_elements_by_class_name('folder-toggle')\n",
    "#     if folders:\n",
    "#         for folder in folders:\n",
    "#             folder.click()\n",
    "#     else:\n",
    "#         print('No folders found for', request_id)\n",
    "    \n",
    "#     doc_links = docs.find_elements_by_class_name('document-link')\n",
    "#     display(\n",
    "#         pd.DataFrame(\n",
    "#             list(zip(get_webelement_text(doc_links), remove_download_from_urls(get_webelement_link(doc_links)))),\n",
    "#             columns=['title', 'link']\n",
    "#         )\n",
    "#     )\n",
    "#     display(\n",
    "#         pd.DataFrame({\n",
    "#             'title': get_webelement_text(doc_links),\n",
    "#             'link': remove_download_from_urls(get_webelement_link(doc_links))\n",
    "#         })\n",
    "#     )\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a5f3b130",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-3d6bb2694692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Parallelized script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscrape_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscrape_record_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msd_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrequest_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parallelized script\n",
    "scrape_request = lambda i: scrape_record_parallel(urls[0], i)\n",
    "sd_requests = Parallel(n_jobs=-1, prefer='threads', verbose=10)(delayed(scrape_request)(request_id) for request_id in request_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a16da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
